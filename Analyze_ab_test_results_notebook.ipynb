{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze A/B Test Results\n",
    "\n",
    "You may either submit your notebook through the workspace here, or you may work from your local machine and submit through the next page.  Either way assure that your code passes the project [RUBRIC](https://review.udacity.com/#!/projects/37e27304-ad47-4eb0-a1ab-8c12f60e43d0/rubric).  **Please save regularly.**\n",
    "\n",
    "This project will assure you have mastered the subjects covered in the statistics lessons.  The hope is to have this project be as comprehensive of these topics as possible.  Good luck!\n",
    "\n",
    "## Table of Contents\n",
    "- [Introduction](#intro)\n",
    "- [Part I - Probability](#probability)\n",
    "- [Part II - A/B Test](#ab_test)\n",
    "- [Part III - Regression](#regression)\n",
    "\n",
    "\n",
    "<a id='intro'></a>\n",
    "### Introduction\n",
    "\n",
    "A/B tests are very commonly performed by data analysts and data scientists.  It is important that you get some practice working with the difficulties of these \n",
    "\n",
    "For this project, you will be working to understand the results of an A/B test run by an e-commerce website.  Your goal is to work through this notebook to help the company understand if they should implement the new page, keep the old page, or perhaps run the experiment longer to make their decision.\n",
    "\n",
    "**As you work through this notebook, follow along in the classroom and answer the corresponding quiz questions associated with each question.** The labels for each classroom concept are provided for each question.  This will assure you are on the right track as you work through the project, and you can feel more confident in your final submission meeting the criteria.  As a final check, assure you meet all the criteria on the [RUBRIC](https://review.udacity.com/#!/projects/37e27304-ad47-4eb0-a1ab-8c12f60e43d0/rubric).\n",
    "\n",
    "<a id='probability'></a>\n",
    "#### Part I - Probability\n",
    "\n",
    "To get started, let's import our libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#We are setting the seed to assure you get the same answers on quizzes as we set up\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.` Now, read in the `ab_data.csv` data. Store it in `df`.  **Use your dataframe to answer the questions in Quiz 1 of the classroom.**\n",
    "\n",
    "a. Read in the dataset and take a look at the top few rows here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0\n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0\n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0\n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0\n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ab_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Use the cell below to find the number of rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294478"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of rows in dataframe\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. The number of unique users in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290584"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of unique users in dataframe\n",
    "df['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "d. The proportion of users converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12104245244060237"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# proportion of users converted\n",
    "# find number of unique users who converted\n",
    "num_converts = df[df['converted']==True]['user_id'].nunique()\n",
    "# divide number of converts over total number of visitors\n",
    "num_converts/df['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "e. The number of times the `new_page` and `treatment` don't match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1928"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of times the landing page is new, but the group is not treatment\n",
    "df.query('(landing_page == \"new_page\") and (group != \"treatment\")')['user_id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id         1965\n",
       "timestamp       1965\n",
       "group           1965\n",
       "landing_page    1965\n",
       "converted       1965\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of times the group is treatment, but the landing page is not new\n",
    "df.query('(group == \"treatment\") and (landing_page != \"new_page\")').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of times new_page and treatment don't match is 1928 + 1965 = 3893."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Do any of the rows have missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id         0\n",
       "timestamp       0\n",
       "group           0\n",
       "landing_page    0\n",
       "converted       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for each column, count the number of missing rows\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` For the rows where **treatment** does not match with **new_page** or **control** does not match with **old_page**, we cannot be sure if this row truly received the new or old page.  Use **Quiz 2** in the classroom to figure out how we should handle these rows.  \n",
    "\n",
    "a. Now use the answer to the quiz to create a new dataset that meets the specifications from the quiz.  Store your new dataframe in **df2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to drop all cases when the landing_page and group columns contradict each other\n",
    "# index those cases  where landing page is new page but group is not treatment\n",
    "i1 = df[((df.landing_page == 'new_page') & (df.group != 'treatment'))].index\n",
    "# then drop those cases\n",
    "df_int = df.drop(i1)\n",
    "# then index those cases where landing page is old page but group is not control\n",
    "i2 = df_int[((df_int.landing_page == 'old_page') & (df_int.group != 'control'))].index\n",
    "# and drop those cases\n",
    "df2 = df_int.drop(i2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double Check all of the correct rows were removed - this should be 0\n",
    "df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what about when group is control and LP isn't old page?\n",
    "# we need zero of those too\n",
    "df2[((df2['group'] == 'control') == (df2['landing_page'] == 'old_page')) == False].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now always true that all treatment pages are new pages, and all control pages are old pages. So we've successfully removed all the inconistencies in that part of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Use **df2** and the cells below to answer questions for **Quiz3** in the classroom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. How many unique **user_id**s are in **df2**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290584"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count number of unique user_ids in current dataframe\n",
    "df2['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "b. There is one **user_id** repeated in **df2**.  What is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of duplicated user ids\n",
    "df2['user_id'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-14 02:55:59.590927</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                   timestamp      group landing_page  converted\n",
       "2893   773192  2017-01-14 02:55:59.590927  treatment     new_page          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the duplicated user_id\n",
    "duplicated = df2[df2.duplicated(['user_id'])]\n",
    "duplicated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. What is the row information for the repeat **user_id**? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-09 05:37:58.781806</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-14 02:55:59.590927</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                   timestamp      group landing_page  converted\n",
       "1899   773192  2017-01-09 05:37:58.781806  treatment     new_page          0\n",
       "2893   773192  2017-01-14 02:55:59.590927  treatment     new_page          0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine rows that have the repeat user's id\n",
    "df_repeats = df2[df2['user_id'] == 773192]\n",
    "df_repeats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Remove **one** of the rows with a duplicate **user_id**, but keep your dataframe as **df2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the second row with the duplicated user_id\n",
    "df2 = df2.drop([2893], axis=0)\n",
    "# confirm that row no longer exists\n",
    "# (should yield the error: [2893] not found in axis)\n",
    "#df2.iloc(2893) [It works, but I'm sick of seeing the error]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Use **df2** in the cells below to answer the quiz questions related to **Quiz 4** in the classroom.\n",
    "\n",
    "a. What is the probability of an individual converting regardless of the page they receive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probablility of conversion of all users in entire dataframe\n",
    "df2.converted.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Given that an individual was in the `control` group, what is the probability they converted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1203863045004612"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probability of conversion of all users in control group\n",
    "df2_control = df2[df2['group']=='control']\n",
    "df2_control.converted.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Given that an individual was in the `treatment` group, what is the probability they converted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11880806551510564"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_treatment = df2[df2['group']=='treatment']\n",
    "df2_treatment.converted.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0015782389853555567"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# difference between conversion rates of treatment and control\n",
    "observed_conv_rate_diff = (df2_treatment.converted.mean() - df2_control.converted.mean())\n",
    "observed_conv_rate_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. What is the probability that an individual received the new page?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145310"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of people who got the new page\n",
    "(df2[df2.landing_page =='new_page']).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290584"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of rows = number of users (now that we've cleaned up the data)\n",
    "df2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5000619442226688"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of users who got the new page divided by the total number of users\n",
    "# yields the probability that an individual received the new page\n",
    "145310/df2.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Consider your results from parts (a) through (d) above, and explain below whether you think there is sufficient evidence to conclude that the new treatment page leads to more conversions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conversion rate for the new page was less than for the old page. And an equal number of experiments were run (about 14500 for each), so they both had plenty of opportunities to prove themselves. In hypothesis tests, we always err on the side of keeping the original system in tact -- it is up to the new idea to prove itself. Since the new page didn't even do as well as the old page, we don't need to consider if the new page did enough better than the old page to justify overturning the old page. We can simply stick with the old page and be done with all this measuring of truth. {But how do we know 14500 trials with the new page and 14500 trials with the old page is enough trials to make a fair assessment? How do we know how many trials is required for that in a given case?}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ab_test'></a>\n",
    "### Part II - A/B Test\n",
    "\n",
    "Notice that because of the time stamp associated with each event, you could technically run a hypothesis test continuously as each observation was observed.  \n",
    "\n",
    "However, then the hard question is do you stop as soon as one page is considered significantly better than another or does it need to happen consistently for a certain amount of time?  How long do you run to render a decision that neither page is better than another?  \n",
    "\n",
    "These questions are the difficult parts associated with A/B tests in general.  \n",
    "\n",
    "\n",
    "`1.` For now, consider you need to make the decision just based on all the data provided.  If you want to assume that the old page is better unless the new page proves to be definitely better at a Type I error rate of 5%, what should your null and alternative hypotheses be?  You can state your hypothesis in terms of words or in terms of **$p_{old}$** and **$p_{new}$**, which are the converted rates for the old and new pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hypothesis is that the new page performs the same or worse than the old page. The alternative hypothesis is that the new page performs better than the old page. We've decided we're comfortable with a Type 1 error rate of 5%.\n",
    "Pùëúùëôùëë  and Pùëõùëíùë§ stand for the old and new conversion rates, respectively.\n",
    "We can state our hypotheses like this:\n",
    "\n",
    "H(0): Pnew - Pold =< 0\n",
    "\n",
    "H(A): Pnew - Pold > 0\n",
    "\n",
    "with an alpha of .05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Assume under the null hypothesis, $p_{new}$ and $p_{old}$ both have \"true\" success rates equal to the **converted** success rate regardless of page - that is $p_{new}$ and $p_{old}$ are equal. Furthermore, assume they are equal to the **converted** rate in **ab_data.csv** regardless of the page. <br><br>\n",
    "\n",
    "Use a sample size for each page equal to the ones in **ab_data.csv**.  <br><br>\n",
    "\n",
    "Perform the sampling distribution for the difference in **converted** between the two pages over 10,000 iterations of calculating an estimate from the null.  <br><br>\n",
    "\n",
    "Use the cells below to provide the necessary parts of this simulation.  If this doesn't make complete sense right now, don't worry - you are going to work through the problems below to complete this problem.  You can use **Quiz 5** in the classroom to make sure you are on the right track.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. What is the **conversion rate** for $p_{new}$ under the null? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The conversion rate of Pnew and Pold are the same under the null.\n",
    "#We've therefore assumed they both have the same conversion rate\n",
    "#as the average conversion rate of all pages in the dataframe \n",
    "\n",
    "# proportion of users converted\n",
    "# first find number of unique users who converted\n",
    "num_converts = df2[df2['converted']==True]['user_id'].nunique()\n",
    "# divide number of converts over total number of visitors\n",
    "num_converts/df2['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. What is the **conversion rate** for $p_{old}$ under the null? <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Under the null hypothesis, Pold's conversion rate is equal to or better than Pnew's conversion rate. So, under the null hypothesis, Pnew's conversion rate is less than or equal to 0.11959708724499628 and Pold's conversion rate is greater than or equal to 0.11959708724499628. Our simulation will assume that the dividing line value of Pold = Pnew = 0.11959708724499628 is true. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. What is $n_{new}$, the number of individuals in the treatment group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145310"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the number of individuals in the treatment group\n",
    "df2[df2['group']=='treatment'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. What is $n_{old}$, the number of individuals in the control group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145274"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the number of individuals in the control group\n",
    "df2[df2['group']=='control'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Simulate $n_{new}$ transactions with a conversion rate of $p_{new}$ under the null.  Store these $n_{new}$ 1's and 0's in **new_page_converted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12025325166884591"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simulate n-new = 145310 transactions with a conversion rate of Pnew under\n",
    "# the null = 0.11959708724499628\n",
    "# first put the numerical values into variables\n",
    "n_new = 145310\n",
    "n_old = 145274\n",
    "p_in_null = 0.11959708724499628\n",
    "# then use random choice to simulate n_new number of random selections of either\n",
    "# 0 or 1 (ie: either not-converted or converted) with a selection rate of p_in_null\n",
    "# But p_in_null is how often the 1 should be chosen. The 0 should be chosen\n",
    "# at a rate of 1 - p_in_null\n",
    "new_page_converted = np.random.choice([0,1], n_new, p=[1 - p_in_null, p_in_null])\n",
    "# the mean of this array gives us a new probability for \n",
    "# a new page's user being converted into a paying user\n",
    "new_page_converted.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Simulate $n_{old}$ transactions with a conversion rate of $p_{old}$ under the null.  Store these $n_{old}$ 1's and 0's in **old_page_converted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12019356526288255"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see notes above in the new_page_converted\n",
    "old_page_converted = np.random.choice([0,1], n_old, p=[1 - p_in_null, p_in_null])\n",
    "old_page_converted.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g. Find $p_{new}$ - $p_{old}$ for your simulated values from part (e) and (f)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.3541176446657004e-05"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the simulated Pnew - Pold is the mean of \n",
    "# new_page_converted - the mean of old_page_converted\n",
    "# it tells us how much better or worse new pages were at \n",
    "# generating converts than old pages were (in our simulation)\n",
    "new_page_converted.mean() - old_page_converted.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h. Create 10,000 $p_{new}$ - $p_{old}$ values using the same simulation process you used in parts (a) through (g) above. Store all 10,000 values in a NumPy array called **p_diffs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.4277868676257204e-06"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bootstrap through the above 10,000 times\n",
    "# create an array that stores all 10,000 P_new - P_old values\n",
    "# (resulting from running the experiment 10,000 times)\n",
    "p_diffs = []\n",
    "for __ in range(10000):\n",
    "    new_page_con = np.random.choice([0,1], n_new, p=[1 - p_in_null, p_in_null])\n",
    "    old_page_con = np.random.choice([0,1], n_old, p=[1 - p_in_null, p_in_null])\n",
    "    p_diffs.append(new_page_con.mean() - old_page_con.mean())\n",
    "# what is the average of the values in this array?\n",
    "# this average tells us how much better or worse new pages were at \n",
    "# generating converts than old pages were (in our simulation)\n",
    "sum(p_diffs)/len(p_diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Plot a histogram of the **p_diffs**.  Does this plot look like what you expected?  Use the matching problem in the classroom to assure you fully understand what was computed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x7f71307cd630>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f71307cd240>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot a histogram of the values in the array we generated\n",
    "# check it against the difference between new_page and old_page\n",
    "# conversion rates from the actual data set\n",
    "\n",
    "plt.hist(p_diffs)\n",
    "plt.axvline(x=observed_conv_rate_diff , color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we assumed the case of the null hypothesis that is closest to the alternative hypothesis (Pnew - Pold = 0), we got a distribution that is quite a bit to the right of the actual difference between the the performance of the new page versus the old page (shown here by the red line).\n",
    "This tells us that the null hypothesis (Pnew - Pold = 0)  is consistent with the new page doing quite a bit better aganist the old page than it was observed to do. (ie: When we assume Pnew - Pold = 0 the vast majority of simulated trials resulted in Pnew doing better than it had actually been observed to do.)\n",
    "But our null hypothesis was that the new page will either\n",
    "do as well (Pnew - Pold = 0 -- what our diagram above depicts) or WORSE than the old page (Pnew - Pold < 0).\n",
    "So we definitely do not want to reject the null hypothesis. Far from doing enough better than parity (Pnew - Pold = 0) to justify switching from the old to the new pages, the new pages did considerably worse than parity (Pnew - Pold < 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j. What proportion of the **p_diffs** are greater than the actual difference observed in **ab_data.csv**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90410000000000001"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what portion of the values in p_diff (new page conversion rate minus old page\n",
    "# conversion rate under the null assumption of zero difference in conversion rates)\n",
    "# are greater than the actual observed difference in our data set.\n",
    "# this is the p-value: the likelihood of getting a value at least extreme as the\n",
    "# one we observed, if the null hypothesis is true\n",
    "(p_diffs > observed_conv_rate_diff).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alternative hypothesis has been clobbered. Assuming the null to be true, \n",
    "we generated a random distribution of the difference in the mean conversion \n",
    "rates of the new versus old pages, and only ten percent of the outcomes we generated were as low or lower than the actual observed difference between the new versus old conversion rates. Since the null assumed that the old pages would either do as well or better than the new pages (meaning the old page conversion rate subtracted from the new page conversion rate would be equal to or less than zero), the null hypothesis was right in 90% our 10,000 experiments. That is to say, our p-value (the probability that if the null hypothesis is true, we will obxerve results at least as extreme as the observed results) is .9. Recall that our alpha was .05 and we don't reject the null unless our p-value is less than our alpha. There is absolutely no reason to reject the null. So we fail to reject the null and we keep the old page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l. We could also use a built-in to achieve similar results.  Though using the built-in might be easier to code, the above portions are a walkthrough of the ideas that are critical to correctly thinking about statistical significance. Fill in the below to calculate the number of conversions for each page, as well as the number of individuals who received each page. Let `n_old` and `n_new` refer the the number of rows associated with the old page and new pages, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17489\n",
      "17264\n"
     ]
    }
   ],
   "source": [
    "# calculate how many successful conversions each group actually had\n",
    "convert_old = df2[df2.group == 'control'].converted.sum()\n",
    "convert_new = df2[df2.group == 'treatment'].converted.sum()\n",
    "# there is no need to change n_old or n_new because n_old is already the \n",
    "# number of treatment rows in df2; and n_old is already the number of group\n",
    "# rows in df2\n",
    "n_old = n_old\n",
    "n_new = n_new\n",
    "print(convert_old)\n",
    "print(convert_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m. Now use `stats.proportions_ztest` to compute your test statistic and p-value.  [Here](https://docs.w3cub.com/statsmodels/generated/statsmodels.stats.proportion.proportions_ztest/) is a helpful link on using the built in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.31092419842\n",
      "0.905058312759\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "# counts is the # of successes in the trials in df2\n",
    "counts = np.array([convert_new, convert_old])\n",
    "# nobs is the number of trials that were carried out in df2\n",
    "nobs = np.array([n_new, n_old])\n",
    "# we can see from the graph above that our experiment is \n",
    "# a one-tailed one. Also, our alternative assumes that Pnew > Pold\n",
    "# therefore we have to use the 'alternative=\"larger\"' parameter\n",
    "# note that we also therefore have to put convert_new ahead of convert_old\n",
    "# in the np.array created for counts above\n",
    "stat, pval = proportions_ztest(counts, nobs, alternative=\"larger\")\n",
    "print(stat)\n",
    "print(pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n. What do the z-score and p-value you computed in the previous question mean for the conversion rates of the old and new pages?  Do they agree with the findings in parts **j.** and **k.**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value from the z-test is a tiny bit greater than the one we found through the simulations (parts h, i, j, k above). But they are close (about .904 for simulations versus about .905 for the ztest) and both clearly argue for failing to reject the null.\n",
    "\n",
    "A z-score of -1.3 means the sample average is 1.3 below the hypothesized mean of zero (ie: of Pnew = Pold). So again the bulk of the simulated trials resulted in values that are less than zero, which is to say values in which Pold is actually larger than Pnew. Again, we fail to reject the null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regression'></a>\n",
    "### Part III - A regression approach\n",
    "\n",
    "`1.` In this final part, you will see that the result you achieved in the A/B test in Part II above can also be achieved by performing regression.<br><br> \n",
    "\n",
    "a. Since each row is either a conversion or no conversion, what type of regression should you be performing in this case?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice between conversion or no-conversion is a categorical one (either category 0 for no conversion or category 1 for conversion). For modeling categorical choices, we have to use logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. The goal is to use **statsmodels** to fit the regression model you specified in part **a.** to see if there is a significant difference in conversion based on which page a customer receives. However, you first need to create in df2 a column for the intercept, and create a dummy variable column for which page each user received.  Add an **intercept** column, as well as an **ab_page** column, which is 1 when an individual receives the **treatment** and 0 if **control**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>ab_drop</th>\n",
       "      <th>ab_page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted  \\\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "\n",
       "   ab_drop  ab_page  \n",
       "0        1        0  \n",
       "1        1        0  \n",
       "2        0        1  \n",
       "3        0        1  \n",
       "4        1        0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are creating a logistic regression model for this A/B Test\n",
    "# first we need to create the dummy variables for treatment group = 0\n",
    "# and control group = 1\n",
    "# the first column will map 'control' to 1 and 'treatment' to 0\n",
    "# so we'll drop that column, and keep only the second, which\n",
    "# will map 'treatment' to 1 and 'control' to 0\n",
    "df2[['ab_drop', 'ab_page']] = pd.get_dummies(df2['group'])\n",
    "df2.drop('ab_drop', 1)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{Why doesn't the ab_drop column get dropped?\n",
    "I asked the dataframe to drop it.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366118\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>       <td>No. Iterations:</td>    <td>6.0000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>     <td>converted</td>    <td>Pseudo R-squared:</td>    <td>0.000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2021-06-30 23:37</td>       <td>AIC:</td>        <td>212780.3502</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>       <td>290584</td>            <td>BIC:</td>        <td>212801.5095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>1</td>         <td>Log-Likelihood:</td>  <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>         <td>290582</td>          <td>LL-Null:</td>      <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>       <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>      <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>-1.9888</td>  <td>0.0081</td>  <td>-246.6690</td> <td>0.0000</td> <td>-2.0046</td> <td>-1.9730</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>-0.0150</td>  <td>0.0114</td>   <td>-1.3109</td>  <td>0.1899</td> <td>-0.0374</td> <td>0.0074</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                          Results: Logit\n",
       "==================================================================\n",
       "Model:              Logit            No. Iterations:   6.0000     \n",
       "Dependent Variable: converted        Pseudo R-squared: 0.000      \n",
       "Date:               2021-06-30 23:37 AIC:              212780.3502\n",
       "No. Observations:   290584           BIC:              212801.5095\n",
       "Df Model:           1                Log-Likelihood:   -1.0639e+05\n",
       "Df Residuals:       290582           LL-Null:          -1.0639e+05\n",
       "Converged:          1.0000           Scale:            1.0000     \n",
       "-------------------------------------------------------------------\n",
       "              Coef.   Std.Err.      z      P>|z|    [0.025   0.975]\n",
       "-------------------------------------------------------------------\n",
       "intercept    -1.9888    0.0081  -246.6690  0.0000  -2.0046  -1.9730\n",
       "ab_page      -0.0150    0.0114    -1.3109  0.1899  -0.0374   0.0074\n",
       "==================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the intercept and create the logistic regression model\n",
    "df2['intercept']=1\n",
    "log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])\n",
    "results=log_mod.fit()\n",
    "results.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.136859558416\n",
      "0.985111939603\n",
      "1.01511306462\n"
     ]
    }
   ],
   "source": [
    "print(np.exp(-1.9888))\n",
    "print(np.exp(-.015))\n",
    "print(1/np.exp(-.015))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ab_page's 1 value is 'treatment'; and it's 0 value 'control'.\n",
    "So for every 1 increase in the number of conversions, there was a .985 increase in the number of treatment trials. That is to say: as conversions went up, 'control' increased more than 'treatment'. This again argues for the null hypothesis, since it assumes that 'treatment' will do as well or (as in this model) worse than 'control'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Use **statsmodels** to instantiate your regression model on the two columns you created in part b., then fit the model using the two columns you created in part **b.** to predict whether or not an individual converts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Softmax Function:\n",
    "\n",
    "y_convert_probability = softmax(Intercept + co-eff * x_feature_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.41553292e-06,   3.41553292e-06,   3.46715209e-06, ...,\n",
       "         3.41553292e-06,   3.41553292e-06,   3.46715209e-06])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(scores):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(scores - np.max(scores))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "Intercept = -1.9888\n",
    "ab_page_coeff = 0.0150\n",
    "x_feature_value_treatment = np.asarray(df2['ab_page'])\n",
    "\n",
    "convert_probability_treatment = softmax(Intercept + ab_page_coeff * x_feature_value_treatment )\n",
    "convert_probability_treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4413457038240232e-06"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_probability_treatment.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366118\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>       <td>No. Iterations:</td>    <td>6.0000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>     <td>converted</td>    <td>Pseudo R-squared:</td>    <td>0.000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2021-06-30 23:46</td>       <td>AIC:</td>        <td>212780.3502</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>       <td>290584</td>            <td>BIC:</td>        <td>212801.5095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>1</td>         <td>Log-Likelihood:</td>  <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>         <td>290582</td>          <td>LL-Null:</td>      <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>       <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>      <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>-2.0038</td>  <td>0.0081</td>  <td>-247.1457</td> <td>0.0000</td> <td>-2.0197</td> <td>-1.9879</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_drop</th>   <td>0.0150</td>   <td>0.0114</td>   <td>1.3109</td>   <td>0.1899</td> <td>-0.0074</td> <td>0.0374</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                          Results: Logit\n",
       "==================================================================\n",
       "Model:              Logit            No. Iterations:   6.0000     \n",
       "Dependent Variable: converted        Pseudo R-squared: 0.000      \n",
       "Date:               2021-06-30 23:46 AIC:              212780.3502\n",
       "No. Observations:   290584           BIC:              212801.5095\n",
       "Df Model:           1                Log-Likelihood:   -1.0639e+05\n",
       "Df Residuals:       290582           LL-Null:          -1.0639e+05\n",
       "Converged:          1.0000           Scale:            1.0000     \n",
       "-------------------------------------------------------------------\n",
       "              Coef.   Std.Err.      z      P>|z|    [0.025   0.975]\n",
       "-------------------------------------------------------------------\n",
       "intercept    -2.0038    0.0081  -247.1457  0.0000  -2.0197  -1.9879\n",
       "ab_drop       0.0150    0.0114     1.3109  0.1899  -0.0074   0.0374\n",
       "==================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what if we run the same experiment with the drop column?\n",
    "# recall that that's the column where treatment is 0 and control is 1\n",
    "# so that column should give us the probability of conversion for the control group\n",
    "log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_drop']])\n",
    "results=log_mod.fit()\n",
    "results.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.46715853e-06,   3.46715853e-06,   3.41553927e-06, ...,\n",
       "         3.46715853e-06,   3.46715853e-06,   3.41553927e-06])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(scores):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(scores - np.max(scores))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "Intercept = -2.0038\n",
    "ab_page_coeff = 0.0150\n",
    "x_feature_value_control = np.asarray(df2['ab_drop'])\n",
    "\n",
    "convert_probability_control = softmax(Intercept + ab_page_coeff * x_feature_value_control )\n",
    "convert_probability_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.441345703824024e-06"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_probability_control.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.4703294725430034e-22"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the difference between the probability of conversion in the treatment vs the control groups\n",
    "convert_probability_treatment.mean() - convert_probability_control.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That number is too small to be considered statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Provide the summary of your model below, and use it as necessary to answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using regression analysis and the softsmax function, we found no statistically significant difference between the conversion rates of the treatment and the null groups. We still fail to reject the null, but it is strange that the Pold is no longer outperforming Pnew (as had happened in all our other models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. What is the p-value associated with **ab_page**? Why does it differ from the value you found in **Part II**?<br><br>  **Hint**: What are the null and alternative hypotheses associated with your regression model, and how do they compare to the null and alternative hypotheses in **Part II**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the p-value associated with ab_page, the p-value is .1899. That is much less than the .9 p-value we'd gotten by running simulations, but it is still far above .05 and allows us to reject the null hypothesis. But what is the null hypothesis in this case? It would be that the ab_page coefficient is equal to or greater than 1. This is because the ab_page coefficient shows how many increases there are in the 'treatment' class for every 1 unit increase of the 'conversion' class. So for the 'treatment' class to do as well or worse than the 'control' class (this is always the experiments null hypothesis), the 'treatment' class needs to incremented by 1 unit or less for every 1 unit increase in the 'conversion' class. Since the ab_page score (.985) is less than 1 and the p-value greater than .05, we can safely fail to reject the null. I am not sure why the p-value was so different in the regression analysis. Both analyses assumed a one-tailed distribution. The null and alternative hypothesis are a little different. Both assume Pnew - Pold = 0 for the null, but the regression model didn't measure directly for that. \n",
    "\n",
    "The regression measured for the coefficient you multiply by the 'treatment' class for every 1 unit increase in the 'conversion' class. This finds essentially the same information as the simulations, but the exponents flatten the results somewhat. Perhaps that squished down the p-value as well? I really don't know.\n",
    "\n",
    "The null and alternative hypotheses in the simulations are Pnew - Pold <= 0 and Pnew - Pold >= 0, respectively.\n",
    "\n",
    "The null hypothesis in the regression model is that the exponent of the intercept in the regression model run for ab_page is less than or equal to 1. That's because the ab_page has 'treatment' as 1, so if the exponent of the intercept is less than or equal to 1, every time you increment 1 conversion class, you increment no more than 1 'treatment' class and at least 1 'control' class. \n",
    "\n",
    "It seems to me that in both cases, you could write the null and alternative hypotheses as Pnew - Pold <= 0 and Pnew - Pold >= 0, respectively. Since though the regression model is going about things in a round about, convoluted and tedious manner, it is still finding out whether or not Pnew does as well, better than, or worse than Pold.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Now, you are considering other things that might influence whether or not an individual converts.  Discuss why it is a good idea to consider other factors to add into your regression model.  Are there any disadvantages to adding additional terms into your regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could look at a user's age, gender, location, how long they stayed on the site, how many times they've visited in the past month. We may not be able to get data for all of those variables, but we should at least be able to track location, how long they stayed on the site, and how many times they used it. Adding extra factors will create confusion if the factors are interrelated. But age, gender, and location should be fairly independent variables. The advantage of adding, say age and gender, might be that we'd discover we were appealing only to a very narrow segment of the population, which -- depending on the content of our site -- we might try to widen. A disadvantage to trying to add more factors is that we could lose sight of the fact that the goal is to maximize the number of users and the number of converts. Also, trying to figure out too precisely who are users and converts are could lead to more errors (since the data would become more complex and less reliable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g. Now along with testing if the conversion rate changes for different pages, also add an effect based on which country a user lives in. You will need to read in the **countries.csv** dataset and merge together your datasets on the appropriate rows.  [Here](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html) are the docs for joining tables. \n",
    "\n",
    "Does it appear that country had an impact on conversion?  Don't forget to create dummy variables for these country columns - **Hint: You will need two columns for the three dummy variables.** Provide the statistical output as well as a written response to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>834778</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>928468</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>822059</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>711597</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>710616</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id country\n",
       "0   834778      UK\n",
       "1   928468      US\n",
       "2   822059      UK\n",
       "3   711597      UK\n",
       "4   710616      UK"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.read_csv('countries.csv')\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>14499</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UK</th>\n",
       "      <td>72466</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US</th>\n",
       "      <td>203619</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  country\n",
       "country                  \n",
       "CA         14499        1\n",
       "UK         72466        1\n",
       "US        203619        1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.groupby('country').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>813256</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>942405</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>867026</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>744693</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>665919</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id country\n",
       "66    813256      CA\n",
       "71    942405      CA\n",
       "73    867026      CA\n",
       "105   744693      CA\n",
       "118   665919      CA"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3[df3['country']=='CA'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>country</th>\n",
       "      <th>UK</th>\n",
       "      <th>US</th>\n",
       "      <th>CA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>834778</td>\n",
       "      <td>UK</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>928468</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>822059</td>\n",
       "      <td>UK</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>711597</td>\n",
       "      <td>UK</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>710616</td>\n",
       "      <td>UK</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id country  UK  US  CA\n",
       "0   834778      UK   0   1   0\n",
       "1   928468      US   0   0   1\n",
       "2   822059      UK   0   1   0\n",
       "3   711597      UK   0   1   0\n",
       "4   710616      UK   0   1   0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3[['UK', 'US', 'CA']] = pd.get_dummies(df3['country'])\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>ab_drop</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>intercept</th>\n",
       "      <th>country</th>\n",
       "      <th>UK</th>\n",
       "      <th>US</th>\n",
       "      <th>CA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted  \\\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "\n",
       "   ab_drop  ab_page  intercept country  UK  US  CA  \n",
       "0        1        0          1      US   0   0   1  \n",
       "1        1        0          1      US   0   0   1  \n",
       "2        0        1          1      US   0   0   1  \n",
       "3        0        1          1      US   0   0   1  \n",
       "4        1        0          1      US   0   0   1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = df2.join(df3.set_index('user_id'), on='user_id')\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366113\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>       <td>No. Iterations:</td>    <td>6.0000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>     <td>converted</td>    <td>Pseudo R-squared:</td>    <td>0.000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2021-06-25 17:31</td>       <td>AIC:</td>        <td>212781.1253</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>       <td>290584</td>            <td>BIC:</td>        <td>212823.4439</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>3</td>         <td>Log-Likelihood:</td>  <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>         <td>290580</td>          <td>LL-Null:</td>      <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>       <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>     <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>-2.0300</td>  <td>0.0266</td>  <td>-76.2488</td> <td>0.0000</td> <td>-2.0822</td> <td>-1.9778</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>-0.0149</td>  <td>0.0114</td>   <td>-1.3069</td> <td>0.1912</td> <td>-0.0374</td> <td>0.0075</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>US</th>        <td>0.0506</td>   <td>0.0284</td>   <td>1.7835</td>  <td>0.0745</td> <td>-0.0050</td> <td>0.1063</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CA</th>        <td>0.0408</td>   <td>0.0269</td>   <td>1.5161</td>  <td>0.1295</td> <td>-0.0119</td> <td>0.0934</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                          Results: Logit\n",
       "==================================================================\n",
       "Model:              Logit            No. Iterations:   6.0000     \n",
       "Dependent Variable: converted        Pseudo R-squared: 0.000      \n",
       "Date:               2021-06-25 17:31 AIC:              212781.1253\n",
       "No. Observations:   290584           BIC:              212823.4439\n",
       "Df Model:           3                Log-Likelihood:   -1.0639e+05\n",
       "Df Residuals:       290580           LL-Null:          -1.0639e+05\n",
       "Converged:          1.0000           Scale:            1.0000     \n",
       "-------------------------------------------------------------------\n",
       "               Coef.   Std.Err.     z      P>|z|    [0.025   0.975]\n",
       "-------------------------------------------------------------------\n",
       "intercept     -2.0300    0.0266  -76.2488  0.0000  -2.0822  -1.9778\n",
       "ab_page       -0.0149    0.0114   -1.3069  0.1912  -0.0374   0.0075\n",
       "US             0.0506    0.0284    1.7835  0.0745  -0.0050   0.1063\n",
       "CA             0.0408    0.0269    1.5161  0.1295  -0.0119   0.0934\n",
       "==================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "df4['intercept']=1\n",
    "log_mod = sm.Logit(df4['converted'], df4[['intercept', 'ab_page', 'US', 'CA']])\n",
    "results=log_mod.fit()\n",
    "results.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.985210455723\n",
      "1.0519020483\n",
      "1.04164375596\n"
     ]
    }
   ],
   "source": [
    "print(np.exp(-.0149))\n",
    "print(np.exp(.0506))\n",
    "print(np.exp(.0408))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this logistic regression model, we got a coefficient of .98521 for the ab_page with a p-value of .192. Since ab_page has 'treatment' set to 1 and 'control' set to 0, this result suggests that the 'control' group is 1 - .985 = .015 more likely to convert than the treatment page. This is in keeping with previous results, which also had the control page doing a tiny bit better than the treatment group.\n",
    "The model also found the following: UK users were the least likely to convert, US users were 1.052 times more likely to convert than UK users (p-value here was .075) and Canadian users were 1.041 times more likely to convert than UK users (p-value .192)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h. Though you have now looked at the individual factors of country and page on conversion, we would now like to look at an interaction between page and country to see if there significant effects on conversion.  Create the necessary additional columns, and fit the new model.  \n",
    "\n",
    "Provide the summary results, and your conclusions based on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new pages of interactions between the pages\n",
    "# and the two countries that aren't the index\n",
    "df5 = df4\n",
    "df5['ab_UK'] = df4['ab_page'] * df4['UK']\n",
    "df5['ab_CA'] = df5['ab_page'] * df5['CA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366118\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>       <td>No. Iterations:</td>    <td>6.0000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>     <td>converted</td>    <td>Pseudo R-squared:</td>    <td>0.000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2021-06-26 13:47</td>       <td>AIC:</td>        <td>212780.3502</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>       <td>290584</td>            <td>BIC:</td>        <td>212801.5095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>1</td>         <td>Log-Likelihood:</td>  <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>         <td>290582</td>          <td>LL-Null:</td>      <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>       <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>      <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>-1.9888</td>  <td>0.0081</td>  <td>-246.6690</td> <td>0.0000</td> <td>-2.0046</td> <td>-1.9730</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>-0.0150</td>  <td>0.0114</td>   <td>-1.3109</td>  <td>0.1899</td> <td>-0.0374</td> <td>0.0074</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                          Results: Logit\n",
       "==================================================================\n",
       "Model:              Logit            No. Iterations:   6.0000     \n",
       "Dependent Variable: converted        Pseudo R-squared: 0.000      \n",
       "Date:               2021-06-26 13:47 AIC:              212780.3502\n",
       "No. Observations:   290584           BIC:              212801.5095\n",
       "Df Model:           1                Log-Likelihood:   -1.0639e+05\n",
       "Df Residuals:       290582           LL-Null:          -1.0639e+05\n",
       "Converged:          1.0000           Scale:            1.0000     \n",
       "-------------------------------------------------------------------\n",
       "              Coef.   Std.Err.      z      P>|z|    [0.025   0.975]\n",
       "-------------------------------------------------------------------\n",
       "intercept    -1.9888    0.0081  -246.6690  0.0000  -2.0046  -1.9730\n",
       "ab_page      -0.0150    0.0114    -1.3109  0.1899  -0.0374   0.0074\n",
       "==================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a regression model of these interactions\n",
    "sm.Logit(df5['converted'], df5[['intercept','ab_page','ab_UK', 'ab_CA']])\n",
    "results2=log_mod.fit()\n",
    "results2.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am told (and accept without understanding) that this model examines the way the pages interact with the countries (this, I am given to understand, is somehow different from the previous regression model which showed how different pages did in different countries). The ab_page coefficient and p-value results here are both the same as above, which suggests to my informants that the interaction between the pages and the countries is of no statistically discernable consequence. This much makes sense to me, although I cannot pretend to understand how a page interacts with a country--if that is indeed different from how well a page does in a given country, which I guess is what I'm being told is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Because we got p-values of much greater than the allowed .05 regardless of whether we tested the hypothesis with simulated trials or with regression models, we fail to reject the null hypothesis and recommend keeping the old version of the web page up, rather than replacing with the new version of the web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a>\n",
    "## Finishing Up\n",
    "\n",
    "> Congratulations!  You have reached the end of the A/B Test Results project!  You should be very proud of all you have accomplished!\n",
    "\n",
    "> **Tip**: Once you are satisfied with your work here, check over your report to make sure that it is satisfies all the areas of the rubric (found on the project submission page at the end of the lesson). You should also probably remove all of the \"Tips\" like this one so that the presentation is as polished as possible.\n",
    "\n",
    "\n",
    "## Directions to Submit\n",
    "\n",
    "> Before you submit your project, you need to create a .html or .pdf version of this notebook in the workspace here. To do that, run the code cell below. If it worked correctly, you should get a return code of 0, and you should see the generated .html file in the workspace directory (click on the orange Jupyter icon in the upper left).\n",
    "\n",
    "> Alternatively, you can download this report as .html via the **File** > **Download as** submenu, and then manually upload it into the workspace directory by clicking on the orange Jupyter icon in the upper left, then using the Upload button.\n",
    "\n",
    "> Once you've done this, you can submit your project by clicking on the \"Submit Project\" button in the lower right here. This will create and submit a zip file with this .ipynb doc and the .html or .pdf version you created. Congratulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results_notebook.ipynb'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
